# Mathematics for Machine Learning: Linear Algebra
**Institution**: Imperial College London  
**Instructors**: Prof. David Dye, Dr. Sam Cooper, Dr. Marc Deisenroth  
**Platform**: Coursera  
**Status**: In Progress ğŸŸ¦  
[ğŸ“„ View Certificate on Coursera](https://www.coursera.org/learn/linear-algebra-machine-learning)



Â· Book used as reference to go further *Linear Algebra Done Right (4th ed.)* â€” Sheldon Axler 

Â· Projects (independent from the certificate)

---

## Purpose
This folder centralizes all my notes, exercises, and projects for the Linear Algebra course.
Feel free to explore it.

---

## Modules of the certificate
- [Module 1: Introduction to Linear Algebra and to Mathematics for Machine Learning](modules/module_1/README.md)  
  Motivation, course logistics, geometric intuition, first vector operations.

- [Module 2: Vectors are Objects that Move Around Space](modules/module_2/README.md)  
  Vector operations, parameter space, geometry in â„â¿.

- [Module 3: Matrices in Linear Algebra â€“ Objects that Operate on Vectors](modules/module_3/README.md)  
  Matrices as operators, systems of equations, matrix multiplication, rank.

- [Module 4: Matrices Make Linear Mappings](modules/module_4/README.md)  
  Linear transformations, composition, change of basis.

- [Module 5: Eigenvalues and Eigenvectors â€“ Application to Data Problems](modules/module_5/README.md)  
  Eigen decomposition, diagonalization, link to PCA and covariance.

---

## Exercises & Notes from the book *Linear Algebra Done Right* (Axler)
**Resolved exercises and notes**

### Chapters to revise first (certificate-critical)
- **Ch. 1 â€“ Vector Spaces** (1Aâ€“1C): definitions, subspaces, direct sums  
  â†’ [Notes](./book_axler/ch01_vector_spaces/README.md) Â· [Exercises 1Aâ€“1C (resolved)](./book_axler/ch01_vector_spaces/exercises_1A_1C.md)

- **Ch. 2 â€“ Finite-Dimensional Vector Spaces** (2Aâ€“2C): span, basis, dimension  
  â†’ [Notes](./book_axler/ch02_finite_dimensional/README.md) Â· [Exercises 2Aâ€“2C (resolved)](./book_axler/ch02_finite_dimensional/exercises_2A_2C.md)

- **Ch. 3 â€“ Linear Maps** (3Aâ€“3D): null space, range, matrix of a map, invertibility, change of basis  
  â†’ [Notes](./book_axler/ch03_linear_maps/README.md) Â· [Exercises 3Aâ€“3D (resolved)](./book_axler/ch03_linear_maps/exercises_3A_3D.md)

- **Ch. 5 â€“ Eigenvalues & Eigenvectors** (5Aâ€“5D): minimal polynomial, upper-triangular form, diagonalizability  
  â†’ [Notes](./book_axler/ch05_eigenvalues/README.md) Â· [Exercises 5Aâ€“5D (resolved)](./book_axler/ch05_eigenvalues/exercises_5A_5D.md)

### Chapters to add for career (quant-oriented depth)
- **Ch. 6 â€“ Inner Product Spaces** (6Aâ€“6C): norms, Gramâ€“Schmidt, orthogonal projections, pseudoinverse â†’ least-squares / regression  
  â†’ [Notes](./book_axler/ch06_inner_product/README.md) Â· [Exercises 6Aâ€“6C (resolved)](./book_axler/ch06_inner_product/exercises_6A_6C.md)

- **Ch. 7 â€“ Operators on Inner Product Spaces** (7Aâ€“7B, 7Eâ€“7F): spectral theorem, SVD, QR/polar â†’ PCA, risk factors, dimensionality reduction  
  â†’ [Notes](./book_axler/ch07_operators/README.md) Â· [Exercises 7Aâ€“7F (resolved)](./book_axler/ch07_operators/exercises_7A_7F.md)

### Exercise sets to solve (tracked as â€œresolvedâ€ in this repo)
- **1Aâ€“1C** â€” fields / vectors / subspaces (proof fluency)  
  â†’ [Exercises 1Aâ€“1C](./book_axler/ch01_vector_spaces/exercises_1A_1C.md)

- **3Bâ€“3C** â€” null/range, rank, matrices of linear maps (bridge to code)  
  â†’ [Exercises 3Bâ€“3C](./book_axler/ch03_linear_maps/exercises_3B_3C.md)

- **5Aâ€“5D** â€” eigenvalues, minimal polynomial, diagonalization (PCA foundation)  
  â†’ [Exercises 5Aâ€“5D](./book_axler/ch05_eigenvalues/exercises_5A_5D.md)

- **6Aâ€“6C** â€” inner products, Gramâ€“Schmidt, projections / pseudoinverse (least-squares)  
  â†’ [Exercises 6Aâ€“6C](./book_axler/ch06_inner_product/exercises_6A_6C.md)

- **7B, 7E** â€” Spectral Theorem & SVD (risk factors / compression)  
  â†’ [Exercises 7B & 7E](./book_axler/ch07_operators/exercises_7B_7E.md)

---

## Projects
| Notebook | Description |
|-----------|-------------|
| [`pca_covariance.ipynb`](projects/pca_covariance.ipynb) | Principal Component Analysis applied to market returns and risk decomposition. |
| [`linear_transformations.ipynb`](projects/linear_transformations.ipynb) | Visualization of eigenvectors, eigenvalues, and transformations in a financial context. |

---

**Course Link**: [Coursera â€“ Mathematics for Machine Learning: Linear Algebra](https://www.coursera.org/learn/linear-algebra-machine-learning)
